// --- START OF FILE chatLLM.ts ---

import {
  Content,
  Part,
  FunctionCall,
  GenerateContentResponse,
} from "@google/generative-ai";
import { connectToMcpServers } from "../../../utils/llmChat/getMcpTools";
import { initializeAndGetModel } from "../../../llm/gemini";
import * as fs from 'fs/promises'; // Import Node.js filesystem promises API
import * as path from 'path'; // Import Node.js path module

// --- Configuration for Logging ---
// Log files will be created in the project root directory (where the Node process is run from)
const LOG_DIRECTORY = process.cwd();

// --- Helper function for delay ---
const delay = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));

// --- Retry configuration ---
const MCP_TOOL_MAX_RETRIES = 3; // Number of retries *after* the initial attempt
const MCP_TOOL_RETRY_DELAY_MS = 1000; // Delay between retries in milliseconds (e.g., 1 second)
const TOTAL_ATTEMPTS = 1 + MCP_TOOL_MAX_RETRIES; // Total number of attempts


/**
 * Asynchronously logs the user query and chatbot response to a model-specific file
 * located in the project's root directory.
 * Appends log entries to the file.
 *
 * @param modelName - The name of the LLM model used (e.g., "gemini-1.5-flash"). Used directly for the filename.
 * @param userQuery - The original query sent by the user.
 * @param chatbotResponse - The final response generated by the chatbot.
 */
const logChatInteraction = async (modelName: string, userQuery: string, chatbotResponse: string): Promise<void> => {
    let logFilePath = ''; // Define filePath outside try for use in catch
    try {
        // 1. Sanitize model name for use as a filename (replace slashes, colons, etc., keep dots/hyphens)
        // Also remove any trailing dots that might cause issues on some systems.
        const sanitizedModelName = modelName.replace(/[^a-z0-9_\-\.]/gi, '_').replace(/\.+$/, '');
        // Ensure a default name if sanitization somehow results in an empty string
        const safeModelName = sanitizedModelName || 'unknown_model';
        const logFileName = `${safeModelName}.log`; // Use model name + .log extension
        logFilePath = path.join(LOG_DIRECTORY, logFileName); // Join root dir and filename

        // 2. Format the Log Entry
        const logEntry = `
============================================================
Model: ${modelName}
------------------------------------------------------------
User Query:
${userQuery}
------------------------------------------------------------
Chatbot Response:
${chatbotResponse}
============================================================
\n`; // Add extra newline for spacing between entries

        // 3. Append to the Log File in the root directory
        // No need to explicitly create LOG_DIRECTORY as process.cwd() should exist
        await fs.appendFile(logFilePath, logEntry, 'utf8');
        console.log(`Chat interaction logged successfully to ${logFilePath}`);

    } catch (logError) {
        console.error(`[Log Error] Failed to write chat interaction to log file '${logFilePath || `${modelName}.log`}':`, logError);
        // Log the error but don't crash the main request handler
    }
};


export const chatWithLLM = async (req: any, res: any) => {
  // --- Extract necessary data early ---
  const { message, history, model: modelName } = req.body; // Renamed 'model' to 'modelName' for clarity

  // --- Input Validation ---
  if (!message) {
    return res.status(400).json({ error: "Message is required" });
  }
  if (!modelName) {
    // Crucial for logging and initialization
    return res.status(400).json({ error: "Model name ('model') is required in the request body" });
  }

  try {
    // --- Tool and MCP Server Setup ---
    const { allGeminiTools, mcpClients, toolToServerMap } = await connectToMcpServers();

    console.log(
      "Tools configured for Gemini:",
      allGeminiTools.map(t => t.name)
    );
    console.log("Connected MCP Clients:", Array.from(mcpClients.keys()));
    console.log("Tool to Server Map:", Object.fromEntries(toolToServerMap));

    if (mcpClients.size === 0 && allGeminiTools.length > 0) { // Only error if tools were expected but no servers connected
     console.warn("Warning: Tools defined but no MCP Servers are connected.");
     // Decide if this should be a blocking error or just a warning
     // return res.status(503).json({ error: "Tools require MCP Servers, but none are connected.", allGeminiTools });
    }

    // --- Initialize LLM ---
    const geminiModel = await initializeAndGetModel(modelName); // Use modelName here
    if (!geminiModel) {
      return res.status(503).json({ error: `Server LLM '${modelName}' not configured or failed to initialize` });
    }

    // --- Start Chat ---
    const chat = geminiModel.startChat({
      history: (history || []) as Content[],
      tools:
        allGeminiTools.length > 0
          ? [{ functionDeclarations: allGeminiTools }]
          : undefined,
    });

    // --- Send Initial Message ---
    let result = await chat.sendMessage(message);
    let response: GenerateContentResponse | undefined = result.response;

    let currentContent: Content | undefined =
      response?.candidates?.[0]?.content;
    let functionCallsToProcess: FunctionCall[] | undefined =
      currentContent?.parts
        ?.filter((part: Part) => !!part.functionCall)
        .map((part: Part) => part.functionCall as FunctionCall);

    // --- Handle Function Calls (Tool Use) ---
    while (functionCallsToProcess && functionCallsToProcess.length > 0) {
      console.log(
        `Gemini requested ${functionCallsToProcess.length} tool call(s):`,
        functionCallsToProcess.map((c) => c.name)
      );

      const functionResponses: Part[] = [];

      // Process tool calls (using Promise.all for potential concurrency)
      await Promise.all(functionCallsToProcess.map(async (call) => {
        const toolName = call.name;
        const toolArgs = call.args;
        const serverKey = toolToServerMap.get(toolName);
        const targetClient = serverKey ? mcpClients.get(serverKey) : undefined;

        if (!targetClient) {
          console.error(
            `❌ Tool "${toolName}" requested by Gemini, but no connected MCP client provides it or the client is down (Server Key: ${serverKey || 'Not Found'}).`
          );
          functionResponses.push({
            functionResponse: {
              name: toolName,
              response: {
                content: `Error: Tool "${toolName}" could not be routed to a server. It might be unavailable or the mapping failed.`,
              },
            },
          });
          return;
        }

        console.log(
          `Attempting MCP tool "${toolName}" via server "${serverKey}" with args:`,
          toolArgs
        );

        // --- Retry Logic for Tool Call ---
        let attempt = 0;
        let success = false;
        let mcpToolResult: any = null;
        let lastToolError: any = null;

        while (attempt < TOTAL_ATTEMPTS && !success) {
          attempt++;
          try {
            console.log(`  [Attempt ${attempt}/${TOTAL_ATTEMPTS}] Calling tool "${toolName}"...`);
            mcpToolResult = await targetClient.callTool({
              name: toolName,
              arguments: toolArgs as { [x: string]: unknown } | undefined,
            });
            console.log(`  [Attempt ${attempt}] SUCCESS for tool "${toolName}". Response:`, mcpToolResult);
            success = true;

          } catch (toolError: any) {
            lastToolError = toolError;
            console.warn(
              `  [Attempt ${attempt}/${TOTAL_ATTEMPTS}] FAILED for tool "${toolName}" via server "${serverKey}". Error:`,
              toolError.message || toolError
            );

            if (attempt < TOTAL_ATTEMPTS) {
              console.log(`    Retrying in ${MCP_TOOL_RETRY_DELAY_MS}ms...`);
              await delay(MCP_TOOL_RETRY_DELAY_MS);
            } else {
               console.error(
                 `❌ Tool "${toolName}" failed after ${TOTAL_ATTEMPTS} attempts. Last error:`,
                 lastToolError
               );
            }
          }
        } // End retry while loop

        // --- Process Tool Call Result ---
        if (success && mcpToolResult) {
          functionResponses.push({
            functionResponse: {
              name: toolName,
              response: {
                content:
                  typeof mcpToolResult.content === "string"
                    ? mcpToolResult.content
                    : JSON.stringify(mcpToolResult.content) ||
                      "Tool executed successfully.",
              },
            },
          });
        } else {
          functionResponses.push({
            functionResponse: {
              name: toolName,
              response: {
                content: `Error executing tool ${toolName} after ${TOTAL_ATTEMPTS} attempts: ${
                  lastToolError?.message || "Unknown error during tool execution"
                }`,
              },
            },
          });
        }
      })); // End Promise.all map function

      // --- Send Tool Responses back to LLM ---
      console.log(
        "Sending tool responses back to Gemini:",
        JSON.stringify(functionResponses)
      );
      result = await chat.sendMessage(functionResponses);
      response = result.response;

      // --- Check for more function calls ---
      currentContent = response?.candidates?.[0]?.content;
      functionCallsToProcess = currentContent?.parts
        ?.filter((part: Part) => !!part.functionCall)
        .map((part: Part) => part.functionCall as FunctionCall);
    } // End while loop for processing function calls

    // --- Extract Final Text Answer ---
    let finalAnswer = "Sorry, I could not generate a text response."; // Default message
    if (response?.candidates?.[0]?.content?.parts) {
        const textParts = response.candidates[0].content.parts
          .filter((part: Part): part is Part & { text: string } => typeof part.text === 'string')
          .map((part) => part.text);
        if (textParts.length > 0) {
          finalAnswer = textParts.join(" ");
        } else if (!response.candidates[0].finishReason || response.candidates[0].finishReason === 'STOP') {
           console.log("Gemini finished, but no final text part was generated.");
           // Keep the default "Sorry..." message or adjust as needed
        }
    } else if (response?.promptFeedback?.blockReason) {
        finalAnswer = `My response was blocked. Reason: ${response.promptFeedback.blockReason}`;
        console.warn(`Gemini response blocked: ${response.promptFeedback.blockReason}`, response.promptFeedback);
    }

    console.log("Final Gemini response being sent to user:", finalAnswer);

    // --- *** LOG THE INTERACTION *** ---
    // Log the original user message and the final computed answer to the root directory
    await logChatInteraction(modelName, message, finalAnswer);
    // ---------------------------------

    // --- Send Response to Client ---
    const finalHistory = await chat.getHistory();
    res.json({ reply: finalAnswer, history: finalHistory });

  } catch (err: any) {
    console.error("Error in chatWithLLM:", err.stack || err);
    // --- Attempt to Log Error Scenario ---
    // Even if the main chat flow failed, try to log what we have to the root dir
    await logChatInteraction(
      modelName || 'unknown_model_on_error', // Use a distinct name if modelName was missing
      message || 'unknown_query',   // Use 'unknown_query' if message wasn't available
      `Error during processing: ${err.message || 'Unknown server error'}` // Log the error message as the response
    );
    // -------------------------------------
    res.status(500).json({ error: `Failed in chat handler: ${err.message || 'Unknown server error'}` });
  }
};
// --- END OF FILE chatLLM.ts ---